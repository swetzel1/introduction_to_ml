{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swetzel1/introduction_to_ml/blob/main/hw5/hw5_p3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EhhoSqeiRsm3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "725f6302-205d-473f-beba-120c4e14488d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "torch.set_printoptions(edgeitems=2, linewidth=75)\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SJ76QEdRsm6"
      },
      "outputs": [],
      "source": [
        "#import and preprocess housing dataset\n",
        "#read data from csv file\n",
        "file_path = '/content/drive/My Drive/Intro_to_ML/Datasets/Housing.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "from IPython.display import display\n",
        "#display(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#select required parameters\n",
        "varlist =  ['price', 'area', 'bedrooms', 'bathrooms', 'stories', 'mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'parking', 'prefarea']\n",
        "\n",
        "df = df[varlist]\n",
        "#df.head()"
      ],
      "metadata": {
        "id": "EmjpNTUBp9_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of variables to map\n",
        "varlist =  ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']\n",
        "\n",
        "# Defining the map function\n",
        "def binary_map(x):\n",
        "    return x.map({'yes': 1, 'no': 0})\n",
        "\n",
        "# Applying the function to the housing list\n",
        "df[varlist] = df[varlist].apply(binary_map)\n",
        "#df.head()"
      ],
      "metadata": {
        "id": "WV5OU1u1fKjT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "num_vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking'] # don't normalize output\n",
        "\n",
        "df[num_vars] = scaler.fit_transform(df[num_vars])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "ZzKT5JsYq3DP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shuffle and prepare split\n",
        "n_samples = df.shape[0]\n",
        "n_val = int(0.2 * n_samples) # 20/80 split\n",
        "\n",
        "shuffled_indices = torch.randperm(n_samples)\n",
        "\n",
        "train_indices = shuffled_indices[:-n_val]\n",
        "val_indices = shuffled_indices[-n_val:]\n",
        "\n",
        "\n",
        "#split training and validation set\n",
        "y = df.pop('price')\n",
        "y = y.to_numpy()\n",
        "x = df.to_numpy()\n",
        "\n",
        "#convert numpy to tensor\n",
        "x_train = torch.tensor(x[train_indices], dtype=torch.float32)\n",
        "y_train = torch.tensor(y[train_indices], dtype=torch.float32)\n",
        "x_val = torch.tensor(x[val_indices], dtype=torch.float32)\n",
        "y_val = torch.tensor(y[val_indices], dtype=torch.float32)\n",
        "\n"
      ],
      "metadata": {
        "id": "TnzJOsWiYiZa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class linearRegression(torch.nn.Module):\n",
        "    def __init__(self, inputSize, outputSize):\n",
        "        super(linearRegression, self).__init__()\n",
        "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "vKAGSnyN3KIj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#try different learning rates\n",
        "\n",
        "inputDim = 11          # dim of x\n",
        "outputDim = 1         # dim of y\n",
        "epochs = 5001\n",
        "\n",
        "model = linearRegression(inputDim, outputDim)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "\n",
        "training_loss_history = []\n",
        "validation_loss_history = []\n",
        "\n",
        "#for lr in [0.0002, 0.001, 0.005, 0.01]: #learning rates for SGD\n",
        "for lr in [0.001, 0.01, 0.1, 0.5]: #learning rates for Adam\n",
        "#for lr in [0.01]:\n",
        "\n",
        "\n",
        "  print('Learning rate:', lr)\n",
        "\n",
        "\n",
        "  #optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "\n",
        "    train_in = Variable(x_train)\n",
        "    train_label = Variable(y_train)\n",
        "    train_label = train_label.unsqueeze(1)\n",
        "\n",
        "    val_in = Variable(x_val)\n",
        "    val_label = Variable(y_val)\n",
        "    val_label = val_label.unsqueeze(1)\n",
        "\n",
        "\n",
        "    optimizer.zero_grad() #important\n",
        "    train_pred = model(train_in)#predict\n",
        "    loss = criterion(train_pred, train_label)#get train loss\n",
        "\n",
        "    with torch.no_grad():\n",
        "      val_pred = model(val_in)\n",
        "      val_loss = criterion(val_pred, val_label)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    #store loss and print\n",
        "    training_loss_history.append(loss.item())#store loss\n",
        "    validation_loss_history.append(val_loss.item())\n",
        "\n",
        "    if epoch % 500 == 0: #report loss every 500 epochs\n",
        "      print('Epoch %d, Training Loss %f, Validation Loss: %f' % (epoch, float(loss), float(val_loss)))\n",
        "\n",
        "\n",
        "\n",
        "  #print('\\n')\n",
        "  #plt.plot(training_loss_history, label=\"Training loss\")\n",
        "  #plt.plot(validation_loss_history, label=\"Validation loss\")\n",
        "  #plt.xlabel(\"epochs\")\n",
        "  #plt.ylabel(\"MSE\")\n",
        "  #plt.legend()\n",
        "  #plt.title(lr)\n",
        "  #plt.show()\n",
        "\n",
        "  print('\\n')\n",
        "  training_loss_history.clear()\n",
        "  validation_loss_history.clear()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQMD45ZNQOPa",
        "outputId": "305fa7fb-5856-4d66-a99e-59a5c954cfc6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate: 0.001\n",
            "Epoch 0, Training Loss 26871444013056.000000, Validation Loss: 23583399084032.000000\n",
            "Epoch 500, Training Loss 26871420944384.000000, Validation Loss: 23583382306816.000000\n",
            "Epoch 1000, Training Loss 26871404167168.000000, Validation Loss: 23583365529600.000000\n",
            "Epoch 1500, Training Loss 26871383195648.000000, Validation Loss: 23583348752384.000000\n",
            "Epoch 2000, Training Loss 26871362224128.000000, Validation Loss: 23583329878016.000000\n",
            "Epoch 2500, Training Loss 26871339155456.000000, Validation Loss: 23583315197952.000000\n",
            "Epoch 3000, Training Loss 26871320281088.000000, Validation Loss: 23583298420736.000000\n",
            "Epoch 3500, Training Loss 26871297212416.000000, Validation Loss: 23583281643520.000000\n",
            "Epoch 4000, Training Loss 26871282532352.000000, Validation Loss: 23583266963456.000000\n",
            "Epoch 4500, Training Loss 26871261560832.000000, Validation Loss: 23583245991936.000000\n",
            "Epoch 5000, Training Loss 26871238492160.000000, Validation Loss: 23583231311872.000000\n",
            "\n",
            "\n",
            "Learning rate: 0.01\n",
            "Epoch 0, Training Loss 26871238492160.000000, Validation Loss: 23583231311872.000000\n",
            "Epoch 500, Training Loss 26871035068416.000000, Validation Loss: 23583063539712.000000\n",
            "Epoch 1000, Training Loss 26870829547520.000000, Validation Loss: 23582893670400.000000\n",
            "Epoch 1500, Training Loss 26870630318080.000000, Validation Loss: 23582730092544.000000\n",
            "Epoch 2000, Training Loss 26870422700032.000000, Validation Loss: 23582560223232.000000\n",
            "Epoch 2500, Training Loss 26870221373440.000000, Validation Loss: 23582390353920.000000\n",
            "Epoch 3000, Training Loss 26870020046848.000000, Validation Loss: 23582222581760.000000\n",
            "Epoch 3500, Training Loss 26869818720256.000000, Validation Loss: 23582056906752.000000\n",
            "Epoch 4000, Training Loss 26869613199360.000000, Validation Loss: 23581887037440.000000\n",
            "Epoch 4500, Training Loss 26869411872768.000000, Validation Loss: 23581719265280.000000\n",
            "Epoch 5000, Training Loss 26869206351872.000000, Validation Loss: 23581551493120.000000\n",
            "\n",
            "\n",
            "Learning rate: 0.1\n",
            "Epoch 0, Training Loss 26869206351872.000000, Validation Loss: 23581551493120.000000\n",
            "Epoch 500, Training Loss 26867178405888.000000, Validation Loss: 23579873771520.000000\n",
            "Epoch 1000, Training Loss 26865144168448.000000, Validation Loss: 23578193952768.000000\n",
            "Epoch 1500, Training Loss 26863112028160.000000, Validation Loss: 23576514134016.000000\n",
            "Epoch 2000, Training Loss 26861079887872.000000, Validation Loss: 23574834315264.000000\n",
            "Epoch 2500, Training Loss 26859049844736.000000, Validation Loss: 23573154496512.000000\n",
            "Epoch 3000, Training Loss 26857021898752.000000, Validation Loss: 23571476774912.000000\n",
            "Epoch 3500, Training Loss 26854987661312.000000, Validation Loss: 23569796956160.000000\n",
            "Epoch 4000, Training Loss 26852957618176.000000, Validation Loss: 23568121331712.000000\n",
            "Epoch 4500, Training Loss 26850929672192.000000, Validation Loss: 23566441512960.000000\n",
            "Epoch 5000, Training Loss 26848897531904.000000, Validation Loss: 23564761694208.000000\n",
            "\n",
            "\n",
            "Learning rate: 0.5\n",
            "Epoch 0, Training Loss 26848891240448.000000, Validation Loss: 23564759597056.000000\n",
            "Epoch 500, Training Loss 26838745219072.000000, Validation Loss: 23556368891904.000000\n",
            "Epoch 1000, Training Loss 26828595003392.000000, Validation Loss: 23547980283904.000000\n",
            "Epoch 1500, Training Loss 26818453176320.000000, Validation Loss: 23539595870208.000000\n",
            "Epoch 2000, Training Loss 26808313446400.000000, Validation Loss: 23531217747968.000000\n",
            "Epoch 2500, Training Loss 26798180007936.000000, Validation Loss: 23522839625728.000000\n",
            "Epoch 3000, Training Loss 26788046569472.000000, Validation Loss: 23514465697792.000000\n",
            "Epoch 3500, Training Loss 26777919422464.000000, Validation Loss: 23506091769856.000000\n",
            "Epoch 4000, Training Loss 26767792275456.000000, Validation Loss: 23497726230528.000000\n",
            "Epoch 4500, Training Loss 26757671419904.000000, Validation Loss: 23489360691200.000000\n",
            "Epoch 5000, Training Loss 26747550564352.000000, Validation Loss: 23480997249024.000000\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}